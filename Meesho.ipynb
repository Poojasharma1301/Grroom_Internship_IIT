{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678e471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa3023d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading page urls from text file\n",
    "categories = pd.read_csv(\"Meesho_urls.txt\" , sep=\",\")\n",
    "categories.drop_duplicates(inplace = True)\n",
    "all_links = categories['link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d962b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf774c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: 'Meesho_product_details.csv'\n"
     ]
    }
   ],
   "source": [
    "#creating a base csv file & adding header\n",
    "import csv\n",
    "try:\n",
    "     with open(\"Meesho_product_details.csv\", 'x',encoding='utf8', newline='') as csvFile:\n",
    "        theWriter = csv.writer(csvFile)\n",
    "        columns = [\"Website\", \"Product_Link\", \"Product_Name\", \"Product_Brand\", \"Product_Category\", \"Size_Avail\", \"Price\",\"MRP\", \n",
    "                   \"Gender\", \"Description\", \"Primary_Image_Links\", \"Secondary_Image_Links\"]\n",
    "        theWriter.writerow(columns)\n",
    "except FileExistsError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "404d0317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Categories to scrape:- 2\n",
      "\n",
      "Scraping for catogory No.18 ==> necklaces\n",
      "page no....1->2->3->4->5->6->7->8->9->10->11->12->13->14->15->16->17->18->19->20->21->22->23->24->25->26->27->28->29->30->31->32->33->34->35->36->37->38->39->40->41->42->43->44->45->46->47->48->49->50->51->52->53->54->55->56->57->58->59->60->61->62->63->64->65->66->67->68->69->70->71->72->73->74->75->76->77->78->79->80->81->82->83->84->85->86->87->88->89->90->91->92->93->94->95->96->97->98->99->100->101->102->103->104->105->106->107->108->109->110->111->112->113->114->115->116->117->118->119->120->121->122->123->124->125->126->127->128->129->130->131->132->133->134->135->136->137->138->139->140->141->142->143->144->145->146->147->148->149->150->151->152->153->154->155->156->157->158->159->160->161->162->163->164->165->166->167->168->169->170->171->172->173->174->175->176->177->178->179->180->181->182->183->184->185->186->187->188->189->190->191->192->193->194->195->196->197->198->199->200->201->202->203->204->205->206->207->208->209->210->211->212->213->214->215->216->217->218->219->220->221->222->223->224->225->226->227->228->229->230->231->232->233->234->235->236->237->238->239->240->241->242->243->244->245->246->247->248->249->250->251->252->253->254->255->256->257->258->259->260->261->262->263->264->265->266->267->268->269->270->271->272->273->274->275->276->277->278->279->280->281->282->283->284->285->286->287->288->289->290->291->292->293->294->295->296->297->298->299->\n",
      "\n",
      "Scraping for catogory No.19 ==> rings\n",
      "page no....1->2->3->4->5->6->7->8->9->10->11->12->13->14->15->16->17->18->19->20->21->22->23->24->25->26->27->28->29->30->31->32->33->34->35->36->37->38->39->40->41->42->43->44->45->46->47->48->49->50->51->52->53->54->55->56->57->58->59->60->61->62->63->64->65->66->67->68->69->70->71->72->73->74->75->76->77->78->79->80->81->82->83->84->85->86->87->88->89->90->91->92->93->94->95->96->97->98->99->100->101->102->103->104->105->106->107->108->109->110->111->112->113->114->115->116->117->118->119->120->121->122->123->124->125->126->127->128->129->130->131->132->133->134->135->136->137->138->139->140->141->142->143->144->145->146->147->148->149->150->151->152->153->154->155->156->157->158->159->160->161->162->163->164->165->166->167->168->169->170->171->172->173->174->175->176->177->178->179->180->181->182->183->184->185->186->187->188->189->190->191->192->193->194->195->196->197->198->199->200->201->202->203->204->205->206->207->208->209->210->211->212->213->214->215->216->217->218->219->220->221->222->223->224->225->226->227->228->229->230->231->232->233->234->235->236->237->238->239->240->241->242->243->244->245->246->247->248->249->250->251->252->253->254->255->256->257->258->259->260->261->262->263->264->265->266->267->268->269->270->271->272->273->274->275->276->277->278->279->280->281->282->283->284->285->286->287->288->289->290->291->292->293->294->295->296->297->298->299->\n",
      "\n",
      "Completed! Total time required: 253.19 Minutes\n"
     ]
    }
   ],
   "source": [
    "start =18\n",
    "end = 20\n",
    "page_no = 1\n",
    "#scrapping data\n",
    "begin = time.time()\n",
    "all_link_list = list(all_links)\n",
    "all_link_list = all_link_list[start:end]\n",
    "all_url_links_counts = start\n",
    "print(\"Total Categories to scrape:-\", len(all_link_list))\n",
    "print()\n",
    "with open(\"Meesho_product_details.csv\", 'a', newline='',encoding='utf8') as csvFile:\n",
    "    theWriter = csv.writer(csvFile)\n",
    "    for cat_url in all_link_list:\n",
    "        #adding page numbers to each category link\n",
    "        final_url = []\n",
    "        for i in range(page_no, 300):\n",
    "            final_url.append(cat_url+str(i))\n",
    "        count = 0\n",
    "        Cat_list = categories.category[all_url_links_counts]\n",
    "        Gender = categories.gender[all_url_links_counts]\n",
    "        print(\"Scraping for catogory No.{} ==> {}\".format(all_url_links_counts,Cat_list))\n",
    "        print(\"page no....\", end='')\n",
    "        #scrapping for page\n",
    "        for url in final_url:\n",
    "            count=count+1\n",
    "            try:\n",
    "                page_source = requests.get(url)\n",
    "                soup=BeautifulSoup(page_source.text,'html.parser')\n",
    "                pager = soup.findAll('div', {'class':'sc-dkPtRN ProductList__GridCol-sc-8lnc8o-0 FjWWx jMkQHN'})\n",
    "                plinks = []\n",
    "                for i in pager:\n",
    "                    plinks.append('https://meesho.com'+i.a['href'])\n",
    "            except:\n",
    "                continue\n",
    "            for pro in plinks:\n",
    "                try:\n",
    "#                     print()\n",
    "#                     print(pro)\n",
    "                    containt2 = requests.get(pro, 'html.parser')\n",
    "                    containt2 = BeautifulSoup(containt2.text, 'html.parser')\n",
    "                    s = containt2.findAll('span',{'class':'SingleChip__StyledChip-sc-oqewox-0 gAAUDP'})\n",
    "                    size = []\n",
    "                    for i in s:\n",
    "                        size.append(i.text)\n",
    "                    size = ','.join(size)\n",
    "                    if size == '':\n",
    "                        size = (containt2.findAll('span', {'class':'SingleChip__StyledChip-sc-oqewox-0 fNHJlp'}))[0].text.strip()\n",
    "#                     print(size)\n",
    "                    pImg = containt2.findAll('div', {'class':'NewProductCarouse__ImageWrapperDesktop-sc-vkv49x-1 jWpEQd'})[0].div.img['src']\n",
    "#                     print(pImg)\n",
    "                    img = containt2.findAll('div',{'class':'Container-sc-wd3hnw-0 kqopSZ'})\n",
    "                    ImgLinks = []\n",
    "                    for i in img:\n",
    "                        Img = i.div.img['src']\n",
    "                        Img = Img.split('_')\n",
    "                        Img[-1] = '512.jpg'\n",
    "                        ImgLinks.append('_'.join(Img))\n",
    "                    secImg = ImgLinks[1:]\n",
    "                    secImg = ','.join(secImg)\n",
    "                    if secImg == '':\n",
    "                        secImg = 'Not Available'\n",
    "#                     print(secImg)\n",
    "                    price = containt2.find('h4', {'class':'Text__StyledText-sc-oo0kvp-0 fyTUEs'}).text.replace('₹','').strip()\n",
    "#                     print(price)\n",
    "                    mrp = containt2.find('p').text.replace('₹','').strip()\n",
    "#                     print(mrp)\n",
    "                    pName = containt2.find('span', {'class':'Text__StyledText-sc-oo0kvp-0 elstub'}).text.strip()\n",
    "#                     print(pName)\n",
    "                    de =containt2.findAll('p', {'class':'Text__StyledText-sc-oo0kvp-0 fCJVtz pre pre'})\n",
    "                    desc = []\n",
    "                    for i in de:\n",
    "                        desc.append(i.text.strip().replace('\\xa0:\\xa0',':'))\n",
    "                    desc = ' '.join(desc)\n",
    "                    description = desc.replace('\\xa0','')\n",
    "#                     print(description)\n",
    "                    pBrand = containt2.find('span', {'class':'Text__StyledText-sc-oo0kvp-0 cGaFci ShopCardstyled__ShopName-sc-du9pku-6 hHfppf ShopCardstyled__ShopName-sc-du9pku-6 hHfppf'}).text.strip()\n",
    "#                     print(pBrand)\n",
    "                    #writing results into csv file\n",
    "                    theWriter.writerow(['https://www.meesho.com',pro,pName,pBrand,Cat_list,size,price,mrp,Gender,description,pImg,secImg])\n",
    "                except Exception:\n",
    "                    pass\n",
    "            print(count,  end='->')\n",
    "        print(end='\\n\\n')\n",
    "        page_no = 1\n",
    "        all_url_links_counts+=1\n",
    "print(\"Completed! Total time required:\",round((time.time()-begin)/60, 2),'Minutes')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6b5280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading csv file\n",
    "product_details_sel = pd.read_csv(\"Meesho_product_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0b7014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('meeshoDb.db')\n",
    "c = conn.cursor()\n",
    "c.execute('CREATE TABLE if not exists product_details (Website varchar(40) NOT NULL,Product_Link TEXT PRIMARY KEY,Product_Name varchar(50) NOT NULL,Product_Brand varchar(50) NOT NULL,Product_Category varchar(50),Size_Avail varchar(20) NOT NULL,Price int NOT NULL,MRP int NOT NULL,Gender varchar(15) NOT NULL,Description TEXT NOT NULL,Primary_Image_Links TEXT NOT NULL,Secondary_Image_Links TEXT NOT NULL,Affiliate_Link TEXT NOT NULL)')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "303db553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserting dataframe into database\n",
    "product_details_sel.to_sql('product_details', conn, if_exists='replace', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9f0a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('https://www.meesho.com', 'https://meesho.com/styles-earrings--studs/p/11p6zv', 'Styles Earrings & Studs', 'SUBHAGALANKAR', 'earrings-women', 'Free Size', 91, 129, 'Women', 'Base Metal:Bentex Plating:Gold Plated Sizing:Adjustable Stone Type:Kundan ', 'https://images.meesho.com/images/products/63321259/xxixd_512.jpg', 'Not Available')\n",
      "('https://www.meesho.com', 'https://meesho.com/vihot-jewellery-oxidised-gold-plated-brass-jhumki-earrings-3416-pink/p/1178pw', 'Vihot Jewellery Oxidised Gold Plated Brass Jhumki Earrings 3416, Pink', 'Vihot Jewellery', 'earrings-women', 'Free Size', 154, 220, 'Women', 'Base Metal:Brass Plating:Oxidised Gold Sizing:Non-Adjustable Stone Type:Artificial Stones & Beads Type:Jhumkhas Multipack:1 Latest Stylish Party Wear Earrings - Vihot Jewellery presents these Stylish Earrings suitable for all occasions Office Wear, Wedding and party wear. A designer and Trendy pair of stylish Earrings for Girls and Women. The color complements all outfits & may be worn as a statement piece to any occasion. The designer artificial jewellery at Vihot Jewellery is available in various designs and patterns like floral design, heart shape design, dancing peacock, crystal design and many more. The jewellery in trend like statement jewellery that is much adored by women of any age a is in trend. The imitation designer jewellery that will go with your outfit and style up your fashion statement with variable designs and patterns to give you charming and elegant look is available at Vihot Jewellery. Vihot Jewellery has huge collection of designer imitation jewellery like American diamond jewellery collection, gold plated jewellery collection, temple jewellery collection, valentine collection, jewellery combos and many more. Country of Origin:India', 'https://images.meesho.com/images/products/62483684/uexoz_512.jpg', 'https://images.meesho.com/images/products/62483684/rtxe1_512.jpg')\n"
     ]
    }
   ],
   "source": [
    "# Print 20 products\n",
    "c.execute('''  \n",
    "SELECT * FROM product_details\n",
    "          ''')\n",
    "\n",
    "for row in c.fetchmany(size=2):\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "605fc4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c42ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BM_1740",
   "language": "python",
   "name": "bm_1740"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
